# -*- coding: utf-8 -*-
"""HR People Analytics.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1m4k85mn64dqKo0z24z6wbzyMug9463-b

✅ PROJECT: HR People Analytics & Attrition Insights Using Employee Attrition Dataset

This project will include:

Data Cleaning & Validation (Python)

Exploratory HR Analytics

Attrition Insights Dashboard (Power BI)

Predictive Attrition Model (Python)

Executive Insights Report (PDF)

GitHub-Ready Project Structure

## Step 1 — Install dependencies
"""

!pip install matplotlib seaborn scikit-learn

"""## Step 2 — Load Data"""

import pandas as pd


train = pd.read_csv("train.csv")
test = pd.read_csv("test.csv")

train.head(), test.head()

"""## Step 3 — Split train.csv into features & target"""

target_col = "Attrition" # Define the target column
X = train.drop(columns=[target_col])
y = train["Attrition"].map({'Left': 1, 'Stayed': 0})

"""## Step 4 — Combine train+test for shared preprocessing"""

combined = pd.concat([X, test], axis=0)

combined.shape

"""## Step 5 — Clean + encode"""

combined = pd.get_dummies(combined, drop_first=True)

X_processed = combined.iloc[:len(train)]
test_processed = combined.iloc[len(train):]

"""## Step 6 — Train/Test split"""

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, roc_auc_score

X_train, X_valid, y_train, y_valid = train_test_split(
    X_processed, y, test_size=0.2, random_state=42
)

"""## Step 7 — Train a model"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, roc_auc_score

model = RandomForestClassifier(
    n_estimators=300,
    max_depth=12,
    random_state=42
)
model.fit(X_train, y_train)

preds = model.predict(X_valid)
probs = model.predict_proba(X_valid)[:,1]

print(classification_report(y_valid, preds))
print("ROC-AUC:", roc_auc_score(y_valid, probs))

"""## Step 8 — Predict on test.csv"""

test_probs = model.predict_proba(test_processed)[:,1]
submission = pd.DataFrame({
    "EmployeeID": test["Employee ID"],
    "Attrition_Probability": test_probs
})
submission.head()

def risk_level(p):
    if p >= 0.70:
        return "High Risk"
    elif p >= 0.40:
        return "Medium Risk"
    else:
        return "Low Risk"

submission["Risk_Level"] = submission["Attrition_Probability"].apply(risk_level)
submission.head()

"""## Step 9 — Save output"""

submission.to_csv("submission.csv", index=False)

files.download("submission.csv")

"""# Feature Importance Insights (Why Are They Leaving?)"""

# For Random Forest:
import pandas as pd
import numpy as np

importance = pd.DataFrame({
    'feature': X_train.columns,
    'importance': model.feature_importances_
}).sort_values('importance', ascending=False)

importance.head(20)

"""## SHAP Explainability
- Global factors driving attrition

- Individual employee explanations

- Summary plots
"""

# Sample from data to run SHAP faster
sample = X_valid.sample(300, random_state=42)  # 300 rows is PLENTY for SHAP

explainer = shap.TreeExplainer(model)
shap_values = explainer.shap_values(sample)

shap.summary_plot(shap_values[1], sample)

!pip install shap

import shap
explainer = shap.TreeExplainer(model)
shap_values = explainer.shap_values(X_valid)

shap.summary_plot(shap_values[1], X_valid)